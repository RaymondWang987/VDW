<section class="bg-primary container statistics-pagepadding" style="background-color:lightgray;width:100%;opacity: 1;" id="download">
    <div class="container" style="padding:0% 2% 8% 2%;background-color:white;width:100%;opacity: 1;">

        <div style="text-align: center;color: rgb(50, 50, 50);max-width: 96%;margin: 1% auto 2% auto;">
            <p style="font-family: Roboto;font-size: 3vw;font-weight: bolder;margin: 0% 1% auto;text-align: left;" class="sec-title">Download</p>
        </div>
        
        <div class="container download-pagepadding" id="metadata">
            <p style="color: #454D64;font-family: Roboto;font-size: 1.6vw;font-weight: bolder;margin:auto;text-align: left;">Download For Metadata</p>
            <p style="margin: 1% auto;text-align: justify;" class="text-justify construct-fullwidthpadding-size">
                Having obtained the raw videos, we use <span style="font-weight: 800;color: #007AFC;"><a href="https://ffmpeg.org/" style="color: #007AFC;">FFmpeg</a></span> and 
                <span style="font-weight: 800;color: #007AFC;"><a href="https://www.scenedetect.com/" style="color: #007AFC;">PySceneDetect</a></span> to split all the videos into 104,582 sequences. 
                We manually check and remove the duplicated, chaotic, and blur scenes. d
                Videos that are wrongly split by the scene detect tools are also removed. 
                Finally, we reserve 32,405 videos with more than six million frames for disparity annotations.
            </p>
        </div>
        <div style="width: 100%;" class="container construct-container construct-margin">
            <div style="margin: auto;" class="construct-item height-md-12 height-sm-12 height-xs-6 col-md-6 col-sm-6 col-xs-12 text-center">
                <img style="max-width: 80%;max-height: 100%;border-radius:2%;" src="statistics_page-0001.png" >
            </div>
            <div style="margin: auto;" class="construct-item height-md-12 height-sm-12 height-xs-6 col-md-6 col-sm-6 col-xs-12 text-center">
                <div style="vertical-align: middle;">
                    <h6 style="color: black;">
                        <span style="font-weight: 1000;font-family: Caveat">Dataset Comparisons</span> 
                    </h6>
                    <h4 style="color: black;margin:2% auto;font-family:Calibri;hyphens: auto;max-width: 100%;text-align: justify;" class="text-justify">
                        VDW dataset has larger numbers of video scenes. 
                        Compared with the closed-domain datasets, VDW is not restricted to a certain scene, which is more helpful to train a robust video depth model. 
                        For the natural-scene datasets, VDW has more than ten times the number of videos as the previous largest dataset <span style="font-weight: 800;color: #007AFC;"><a href="https://sites.google.com/view/wsvd/home" style="color: #007AFC;">WSVD</a></span>. 
                        It is also worth noticing that our VDW dataset has higher resolutions. We only collect videos over 1080p and crop them to 1880 Ã— 800 to remove black bars and subtitles. 
                    </h4>
                </div>
            </div>
        </div>
        
        <div style="width: 100%;" class="container construct-container construct-margin">
            <div style="margin: auto;" class="construct-item height-md-12 height-sm-12 height-xs-6 col-md-6 col-sm-6 col-xs-12 text-center">
                <div style="vertical-align: middle;">
                    <h6 style="color: black;">
                        <span style="font-weight: 1000;font-family: Caveat">Objective Statistics</span> 
                    </h6>
                    <h4 style="color: black;margin:2% auto;font-family:Calibri;hyphens: auto;max-width: 90%;text-align: justify;" class="text-justify">
                        VDW contains 14,203 videos with 2,237,320 frames. The total data collection and processing time takes over six months and about 4,000 man-hours. To verify the diversity of scenes and entities in our dataset, we conduct semantic segmentation by 
                        <span style="font-weight: 800;color: #007AFC;"><a href="https://github.com/facebookresearch/Mask2Former" style="color: #007AFC;">Mask2Former</a></span> trained on <span style="font-weight: 800;color: #007AFC;"><a href="https://groups.csail.mit.edu/vision/datasets/ADE20K/" style="color: #007AFC;">ADE20k</a></span>. 
                        All the 150 categories are covered in our dataset, and each category can be found in at least 50 videos. The five categories that present most frequently are person (97.2%), wall (89.1%), floor (63.5%), ceiling (46.5%), and tree (42.3%). 
                    </h4>
                </div>
            </div>
            <div style="margin: auto;" class="construct-item height-md-12 height-sm-12 height-xs-6 col-md-6 col-sm-6 col-xs-12 text-center">
                <img style="max-width: 80%;max-height: 100%;border-radius:2%" src="statistics_page-0002.png" >
            </div>
        </div>

        <div style="width: 100%;" class="container construct-container">
            <div style="margin: auto;" class="construct-item height-md-12 height-sm-12 height-xs-6 col-md-6 col-sm-6 col-xs-12 text-center">
                <img style="max-width: 60%;max-height: 60%;border-radius:2%;" src="statistics_page-0003.png" >
                <h6 style="color: black;margin-top:1%;">
                    <span style="font-weight: 1000;font-family: Caveat">VDW Training Set</span> 
                </h6>
            </div>
            <div style="margin: auto;" class="construct-item height-md-12 height-sm-12 height-xs-6 col-md-6 col-sm-6 col-xs-12 text-center">
                <div style="vertical-align: middle;">
                    <h6 style="color: black;">
                        <span style="font-weight: 1000;font-family: Caveat">Train&nbsp;/&nbsp;Test Split</span> 
                    </h6>
                    <h4 style="color: black;margin:2% auto 4% auto;font-family:Calibri;hyphens: auto;max-width: 90%;text-align: justify;" class="text-justify">
                        We randomly adopts 104 videos with 13,963 frames as the test set. The testing videos adopt different data sources from the training data, i.e., different movies, web videos, or animations. Our VDW not only alleviates the data shortage for learning-based approaches, but also serves as a comprehensive benchmark for video depth.
                    </h4>
                    <img style="max-width: 80%;max-height: 100%;border-radius:2%;" src="statistics_page-0004.png" >
                    <h6 style="color: black;margin-top:1%;">
                        <span style="font-weight: 1000;font-family: Caveat">VDW Testing Set</span> 
                    </h6>
                </div>
            </div>
        </div>
        
    
    </div>
        

</section>


