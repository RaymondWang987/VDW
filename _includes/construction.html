<section class="bg-primary" style="background-color:#E6E7E8;opacity: 1;height: 200%;padding:3% 10% 3% 10%;" id="construction">
    <!-- <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 text-center">
                <h2 class="section-heading">We've got what you need!</h2>
                <hr class="light">
                <p class="text-faded">Start Bootstrap has everything you need to get your new website up and running in no time! All of the templates and themes on Start Bootstrap are open source, free to download, and easy to use. No strings attached!</p>
                <a href="#" class="btn btn-default btn-xl">Get Started!</a>
            </div>
        </div>
    </div> -->

    <div style="margin: auto;" class="height-md-2 height-sm-2 height-xs-1">
        <div class="height-md-12 height-sm-12 height-xs-6 col-md-6 col-sm-6 col-xs-12 text-center">
            <!-- <img style="max-width: 80%;max-height: 90%;" src="construct_page-0001.jpg" > -->
            <img style="max-width: 100%;max-height: 100%;border-radius:2%" src="construct_page-0001.jpg" >
        </div>
        <div class="height-md-12 height-sm-12 height-xs-6 col-md-6 col-sm-6 col-xs-12 text-center">
            <div style="vertical-align: middle;">
                <h6 style="color: black;margin-top:0px;">
                    <span style="font-weight: 1000;font-family: Caveat">Overall Description</span> 
                </h6>
                <h4 style="color: black;text-align: left;margin-top:4%;font-family:Calibri;">
                    Current video depth datasets are limited in both diversity and volume. 
                    To compensate for the data shortage and boost the performance of learning-based video depth models, 
                    we elaborate a large-scale natural-scene dataset, 
                    <span style="font-weight: 800;">Video Depth in the Wild (VDW)</span>. 
                    To the best of our knowledge, our VDW dataset is currently 
                    <span style="font-weight: 800;">the largest</span> video depth dataset with <span style="font-weight: 600;">the most diverse</span> video scenes.
                </h4>
            </div>
        </div>
    </div>
    
    <div style="margin-top: 4%;margin: auto;" class="height-md-2 height-sm-2 height-xs-1">
        <div class="height-md-12 height-sm-12 height-xs-6 col-md-6 col-sm-6 col-xs-12 text-center">
            <div style="vertical-align: middle;">
                <h6 style="color: black;margin-top:0px;">
                    <span style="font-weight: 1000;font-family: Caveat">Raw Video Acquisition</span> 
                </h6>
                <h4 style="color: black;text-align: left;margin-top:4%;font-family:Calibri;">
                    We collect stereo videos from four data sources: 
                    <span style="font-weight: 800;">movies, animations, documentaries, and web videos. </span>
                    A total of 60 movies, animations, and documentaries in Blu-ray format are collected. 
                    We also crawl 739 web stereo videos from YouTube with the keywords such as “stereoscopic” and “stereo”. 
                    To balance the realism and diversity, only 24 movies, animations, and documentaries are retained. 
                </h4>
            </div>
        </div>
        <div class="height-md-12 height-sm-12 height-xs-6 col-md-6 col-sm-6 col-xs-12 text-center">
            <img style="max-width: 100%;max-height: 100%;border-radius:2%" src="construct_page-0002.png" >
        </div>
    </div>
    



    <div style="margin-top: 4%;margin-bottom: 1%;margin:auto" class="height-md-1 height-sm-1 height-xs-1 text-center">
        <h6 style="color: black;margin-top:0%;">
            <span style="font-weight: 1000;font-family: Caveat">Data Pre-processing</span> 
        </h6>
        <h4 style="color: black;text-align: center;margin-top:1%;font-family:Calibri;max-width: 100%;margin: auto;">
            Having obtained the raw videos, we use <span style="font-weight: 800;color: blue;"><a href="https://ffmpeg.org/" style="color: blue;">FFmpeg</a></span> and 
            <span style="font-weight: 800;color: blue;"><a href="https://www.scenedetect.com/" style="color: blue;">PySceneDetect</a></span> to split all the videos into 104,582 sequences. 
            We manually check and remove the duplicated, chaotic, and blur scenes. 
            Videos that are wrongly split by the scene detect tools are also removed. 
            Finally, we reserve 32,405 videos with more than six million frames for disparity annotations.
        </h4>
    </div>


    <div style="margin-top: 4%;margin: auto;" class="height-md-2 height-sm-2 height-xs-1-5">
        <div class="height-md-12 height-sm-12 height-xs-6 col-md-6 col-sm-6 col-xs-12 text-center">
            <div style="vertical-align: middle;">
                <h6 style="color: black;margin-top:0px;">
                    <span style="font-weight: 1000;font-family: Caveat">Sky Segmentation</span> 
                </h6>
                <h4 style="color: black;text-align: left;margin-top:4%;font-family:Calibri;">
                    As sky is infinitely far, pixels in sky regions should be set to the minimum value in disparity maps. 
                    We generate sky masks in a model ensemble manner. 
                    Each frame along with its horizontally flipped copy are fed into two semantic segmentation models 
                    <span style="font-weight: 800;color: blue;"><a href="https://github.com/NVlabs/SegFormer" style="color: blue;">SegFormer</a></span> and 
                    <span style="font-weight: 800;color: blue;"><a href="https://github.com/facebookresearch/Mask2Former" style="color: blue;">Mask2Former</a></span>, which yields four sky masks in total. 
                    A pixel is considered as the sky when it is positive in more than two predicted sky masks. 
                    Besides, we also fill the connected regions with less than 50 pixels to further remove noisy holes in sky masks. 
    
                </h4>
            </div>
        </div>
        <div class="height-md-12 height-sm-12 height-xs-6 col-md-6 col-sm-6 col-xs-12 text-center">
            <img style="max-width: 100%;max-height: 100%;border-radius:2%" src="construct_page-0003.jpg" >
        </div>
    </div>
    

    <div style="margin-top: 4%;margin: auto;" class="height-md-2 height-sm-2 height-xs-1">
        <div class="height-md-12 height-sm-12 height-xs-6 col-md-6 col-sm-6 col-xs-12 text-center">
            <img style="max-width: 100%;max-height: 100%;border-radius:2%" src="construct_page-0004.jpg" >
        </div>
        <div class="height-md-12 height-sm-12 height-xs-6 col-md-6 col-sm-6 col-xs-12 text-center">
            <div style="vertical-align: middle;">
                <h6 style="color: black;margin-top:0px;">
                    <span style="font-weight: 1000;font-family: Caveat">Disparity Annotation</span> 
                </h6>
                <h4 style="color: black;text-align: left;margin-top:4%;font-family:Calibri;">
                    We adopt the optical flow model <span style="font-weight: 800;color: blue;"><a href="https://github.com/haofeixu/gmflow"  style="color: blue;">GMFlow</a></span> to generate the ground truth disparity of the left- and right-eye views. 
                    The estimated optical flow is bidirectional. We perform a consistency check between the optical flow pairs to obtain the valid masks for training. 
                    We adopt an adaptive consistency threshold for each pixel. The ground truth of each video is normalized by its minimum and maximum disparity. 
                    The disparity value is discretized into 65,535 intervals. 
                </h4>
            </div>
        </div>
    </div>
    

    <div style="margin-top: 4%;" class="height-md-1 height-sm-1 height-xs-1 col-md-12 col-sm-12 col-xs-12 text-center">
        <h6 style="color: black;">
            <span style="font-weight: 1000;font-family: Caveat">Invalid Sample Filtering</span> 
        </h6>
        <h4 style="color: black;text-align: center;margin-top:1%;font-family:Calibri;max-width: 85%;margin: auto;">
            We further filter the videos that are not qualified for our dataset. According to optical flow and valid masks, samples with the following three conditions are removed: 1) more than 30% of pixels in the consistency masks are invalid; 2) more than 10% of pixels have vertical disparity larger than two pixels; 3) the average range of horizontal disparity is less than 15 pixels. Then, we manually check all the videos along with their corresponding ground truth, and remove the samples with obvious errors. Finally, we retain 14,203 videos with 2,237,320 frames in VDW dataset.
        </h4>
    </div>
    


</section>
