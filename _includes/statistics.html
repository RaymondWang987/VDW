<section class="bg-primary container statistics-pagepadding" style="background-color:lightgray;width:100%;opacity: 1;" id="statistics">
    <!-- <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 text-center">
                <h2 class="section-heading">We've got what you need!</h2>
                <hr class="light">#E6E7E8
                <p class="text-faded">Start Bootstrap has everything you need to get your new website up and running in no time! All of the templates and themes on Start Bootstrap are open source, free to download, and easy to use. No strings attached!</p>
                <a href="#" class="btn btn-default btn-xl">Get Started!</a>
            </div>
        </div>
    </div> -->
    <div class="container" style="padding:0% 2% 5% 2%;background-color:white;width:100%;opacity: 1;">
        <!-- <div style="margin: auto;" class="height-md-2 height-sm-2 height-xs-1"> -->
        <div style="width: 100%;" class="container construct-container construct-margin">
            <div style="margin: auto;" class="construct-item height-md-12 height-sm-12 height-xs-6 col-md-6 col-sm-6 col-xs-12 text-center">
                <!-- <img style="max-width: 100%;max-height: 100%;border-radius:2%;background-color: lightgray;" src="construct_page-0001.svg" > -->
                <img style="max-width: 80%;max-height: 100%;border-radius:2%;" src="statistics_page-0001.png" >
            </div>
            <div style="margin: auto;" class="construct-item height-md-12 height-sm-12 height-xs-6 col-md-6 col-sm-6 col-xs-12 text-center">
                <div style="vertical-align: middle;">
                    <h6 style="color: black;">
                        <span style="font-weight: 1000;font-family: Caveat">Dataset Comparisons</span> 
                    </h6>
                    <h4 style="color: black;margin:2% auto;font-family:Calibri;hyphens: auto;max-width: 100%;text-align: justify;" class="text-justify">
                        VDW dataset has larger numbers of video scenes. 
                        Compared with the closed-domain datasets, VDW is not restricted to a certain scene, which is more helpful to train a robust video depth model. 
                        For the natural-scene datasets, VDW has more than ten times the number of videos as the previous largest dataset <span style="font-weight: 800;color: #007AFC;"><a href="https://sites.google.com/view/wsvd/home" style="color: #007AFC;">WSVD</a></span>. 
                        It is also worth noticing that our VDW dataset has higher resolutions. We only collect videos over 1080p and crop them to 1880 × 800 to remove black bars and subtitles. 
                    </h4>
                </div>
            </div>
        </div>
        
        <div style="width: 100%;" class="container construct-container construct-margin">
            <div style="margin: auto;" class="construct-item height-md-12 height-sm-12 height-xs-6 col-md-6 col-sm-6 col-xs-12 text-center">
                <div style="vertical-align: middle;">
                    <h6 style="color: black;">
                        <span style="font-weight: 1000;font-family: Caveat">Objective Statistics</span> 
                    </h6>
                    <h4 style="color: black;margin:2% auto;font-family:Calibri;hyphens: auto;max-width: 90%;text-align: justify;" class="text-justify">
                        VDW contains 14,203 videos with 2,237,320 frames. The total data collection and processing time takes over six months and about 4,000 man-hours. To verify the diversity of scenes and entities in our dataset, we conduct semantic segmentation by 
                        <span style="font-weight: 800;color: #007AFC;"><a href="https://github.com/facebookresearch/Mask2Former" style="color: #007AFC;">Mask2Former</a></span> trained on <span style="font-weight: 800;color: #007AFC;"><a href="https://groups.csail.mit.edu/vision/datasets/ADE20K/" style="color: #007AFC;">ADE20k</a></span>. 
                        All the 150 categories are covered in our dataset, and each category can be found in at least 50 videos. The five categories that present most frequently are person (97.2%), wall (89.1%), floor (63.5%), ceiling (46.5%), and tree (42.3%). 
                    </h4>
                </div>
            </div>
            <div style="margin: auto;" class="construct-item height-md-12 height-sm-12 height-xs-6 col-md-6 col-sm-6 col-xs-12 text-center">
                <img style="max-width: 80%;max-height: 100%;border-radius:2%" src="statistics_page-0002.png" >
            </div>
        </div>
    
        <!-- <div style="margin-top: 4%;" class="height-md-2 height-sm-2 height-xs-1"> -->
        <div style="width: 100%;" class="container construct-container">
            <div style="margin: auto;" class="construct-item height-md-12 height-sm-12 height-xs-6 col-md-6 col-sm-6 col-xs-12 text-center">
                <img style="max-width: 60%;max-height: 60%;border-radius:2%;" src="statistics_page-0003.png" >
                <h6 style="color: black;margin-top:1%;">
                    <span style="font-weight: 1000;font-family: Caveat">VDW Training Set</span> 
                </h6>
            </div>
            <div style="margin: auto;" class="construct-item height-md-12 height-sm-12 height-xs-6 col-md-6 col-sm-6 col-xs-12 text-center">
                <div style="vertical-align: middle;">
                    <h6 style="color: black;">
                        <span style="font-weight: 1000;font-family: Caveat">Train&nbsp;/&nbsp;Test Split</span> 
                    </h6>
                    <h4 style="color: black;margin:2% auto 4% auto;font-family:Calibri;hyphens: auto;max-width: 90%;text-align: justify;" class="text-justify">
                        We randomly adopts 104 videos with 13,963 frames as the test set. The testing videos adopt different data sources from the training data, i.e., different movies, web videos, or animations. Our VDW not only alleviates the data shortage for learning-based approaches, but also serves as a comprehensive benchmark for video depth.
                    </h4>
                    <img style="max-width: 80%;max-height: 100%;border-radius:2%;" src="statistics_page-0004.png" >
                    <h6 style="color: black;margin-top:1%;">
                        <span style="font-weight: 1000;font-family: Caveat">VDW Testing Set</span> 
                    </h6>
                </div>
            </div>
        </div>




        <!-- ----------------- -->
        <div style="text-align: center;color: black;">
            <p style="font-family: Mynerve;font-size: 2.5vw;font-weight: bolder;margin-top: 1% auto">statistics</p>
        </div>

        <div class="tab_contents">
            <ul class="tab_nav">
                <li class="tabNav_active">
                    <h5>NVDS <br> Framework</h5>
                </li>
                <li>
                    <h5>Visual <br> Comparisons</h5>
                </li>
                <li>
                    <h5>Quantitative <br> Results</h5>
                </li>
            </ul>
            <ul class="tab_box" style="color: black;">
                <li class="tabBox_active">
                    <div>
                        <p>Overview of the neural video depth stabilizer. Our framework consists of a depth predictor and a stabilization network. The depth predictor can be any single-image depth model which produces initial flickering disparity maps. Then, the stabilization network refines the flickering disparity maps into temporally consistent ones. The stabilization network functions in a sliding window manner: the frame to be predicted fetches information from adjacent frames for stabilization. During inference, our NVDS framework can be directly adapted to any off-the-shelf depth predictors in a plug-and-play manner. We also devise bidirectional inference to further improve consistency.</p>
                        <!-- <div style="text-align: center;"><a href="img/framework.png" target="_blank"><img src="img/framework.png" alt="Framework" width="60%"></a></div> -->
                        <div style="text-align: center;"><img src="statistics_page-0001.png" alt="Framework" width="100%"></div>
                    </div>
                </li>
                <li>
                    <div>
                        <p>Visual comparisons. DeepV2D and Robust-CVD  show obvious artifacts in those videos. We draw the scanline slice over time; fewer zigzagging pattern means better consistency. Compared with the other video depth methods, our NVDS is more robust on natural scenes and achieves better spatial accuracy and temporal consistency.</p>
                        <!-- <div style="text-align: center;"><a href="img/error_analysis.png" target="_blank"><img src="img/error_analysis.png" alt="Error Analysis" width="100%"></a></div> -->
                        <div style="text-align: center;"><img src="statistics_page-0002.png" alt="Error Analysis" width="100%"></div>
                    </div>
                </li>
                <li>
                    <div>
                        <p>Comparisons with the state-of-the-art approaches. We report the total time of processing eight 640 × 480 frames by different methods on one NVIDIA RTX A6000 GPU. Best performance is in boldface. Second best is underlined.</p>
                        <!-- <div style="text-align: center;"><a href="img/neural_renderer.png" target="_blank"><img src="img/neural_renderer.png" alt="Neural Renderer" width="100%"></a></div> -->
                        <div style="text-align: center;"><img src="statistics_page-0004.png" alt="Neural Renderer" width="100%"></div>
                    </div>
                </li>
            </ul>
        </div>
    
    </div>
        

</section>


